#import "template.typ": *
// Take a look at the file `template.typ` in the file panel
// to customize this template and discover how it works.
#show: project.with(
  title: "高级统计优化算法课程报告",
  subtitle:"蒙特卡洛算法在博弈决策中的应用——以多子连珠游戏为例",
  authors: (
    (name: "213191763 赵子奇", email: "ziqi_zhao@qq.com"),
  ),
)

= 问题背景及简述

== 博弈决策和蒙特卡洛算法

#hide("缩进")博弈决策是指在多方参与的竞争或合作环境中做出最优决策的问题。在博弈中，每个参与者的决策会影响其他参与者的利益，因此需要考虑对手的行为和可能的后果。蒙特卡洛算法通过模拟多次随机事件的结果来估计决策的潜在收益，并根据统计结果做出决策。

蒙特卡洛算法在博弈决策中的应用非常广泛，它是一种基于随机模拟和统计方法的算法，能够在不完全信息的情况下进行决策。在博弈决策中，蒙特卡洛算法的应用主要包括以下几个方面：

+ 状态评估：通过随机模拟游戏的进行来评估不同状态下的收益或效用。通过大量的模拟实验，算法可以估计每个状态的价值，从而帮助决策者选择最优策略。

+ 最优策略搜索：蒙特卡洛树搜索是一种基于蒙特卡洛算法的搜索算法，在博弈决策中得到广泛应用。它通过不断扩展搜索树，并根据蒙特卡洛模拟结果来指导搜索方向，从而找到最优的决策路径。

== 多子连珠游戏

#hide("缩进")多子连珠游戏是一类非常受欢迎的策略性棋盘游戏，其中包括井字棋和五子棋。它们都属于连珠类游戏，目标是通过在棋盘上放置棋子，使自己的棋子在水平、垂直或对角线上连成一条线，从而取得胜利。通过游戏复杂程度的不同，较为流行的多子连珠游戏是井字棋和五子棋。

#box(
 columns(2, gutter: 11pt)[
  #figure(
    image("figs/井字棋.png",height: 21%,fit: "contain"),
    caption: [井字棋],
  )
  #colbreak()
  #figure(
    image("figs/五子棋.jpg",fit: "contain"),
    caption: [五子棋],
  )
 ]
)

#hide("缩进")井字棋是一种简单而受欢迎的两人棋盘游戏。它通常在一个3x3的方格棋盘上进行。两名玩家轮流放置自己的棋子，一个玩家使用“X”，另一个玩家使用“O”。玩家目标是在横、竖或对角线上先连成三个自己的棋子，如果棋盘填满而没有连成线，则为平局。

五子棋是一种更复杂且具有挑战性的棋盘游戏。它通常在一个15x15的方格棋盘上进行，但也可以在其他大小的棋盘上进行。两名玩家轮流放置自己的棋子，一个玩家使用黑色棋子，另一个玩家使用白色棋子。玩家的目标是先在横、竖或对角线上连成五个自己的棋子，以取得胜利。五子棋相对于井字棋来说，棋盘更大，可供选择的落子位置更多，策略和计算的复杂性更高。

这两个游戏都涉及到深思熟虑的决策和战略规划。由于游戏规则相对简单，但又允许广泛的棋局变化，它们成为了经典的博弈决策研究对象。蒙特卡洛算法可以应用于这两个游戏中，通过大量的模拟实验和状态评估，帮助玩家制定更优的决策策略，提高胜率。

== 问题简述
#hide("缩进")本实验的目标是利用蒙特卡洛树算法构建一种辅助决策人工智能，以解决多子连珠游戏中的决策问题。该辅助决策AI将利用蒙特卡洛树搜索算法的核心思想，即通过随机模拟游戏的进行来估计不同状态下的收益或效用，并根据统计结果进行决策。具体而言，通过不断扩展搜索树并使用随机模拟技术，我们将能够模拟出多个可能的游戏局面，并评估每个局面的优劣。这将有助于辅助决策AI根据当前局势和可能的对手行为，推断最佳的决策策略。

= 文献综述

#hide("缩进")蒙特卡洛搜索树算法（Monte Carlo Tree Search, MCTS）在棋类人工智能领域中被广泛研究和应用。其发展主要分为两个阶段。

Browne等人提供了关于MCTS方法的全面综述，概述了其原理、发展历程和不同变体并详细讨论了MCTS在棋类游戏中的应用。研究表明，MCTS在各种棋类游戏中取得了显著的成功#cite("browne2012survey")。该综述论文对MCTS在棋类AI领域的发展和应用提供了深入的了解。

在MCTS的发展过程中的第一个阶段，许多研究致力于将其与先验知识相结合，以提高决策AI的性能。Gelly和Silver提出了一种称为UCT（Upper Confidence bounds applied to Trees）的算法，它能够有效地利用先验知识来引导搜索#cite("gelly2007combining")。该方法在围棋、五子棋和国际象棋等游戏中取得了优秀的成绩。此外，研究人员还关注MCTS中的选择性和备份操作，以提高搜索的效率和准确性。Coulom提出了一种改进的选择动作策略，优化了搜索过程，使得MCTS在棋类游戏中表现出更高的性能#cite("coulom2007efficient")。Silver和Tesauro探讨了蒙特卡洛模拟平衡的重要性，通过调整模拟的平衡性，提高了搜索的效果#cite("silver2009monte")。除此之外，Coulom还提出了一种基于贝叶斯统计的整体历史评分系统，用于评估在时间变化的强度下的棋手。这种评分系统能够更准确地反映棋手的实力，并为MCTS算法中的对手建模提供了一种有效的方法#cite("Coulom2008Rating")。

与此同时，Kamil等人研究了将MCTS算法并行化在图形处理器（GPU）上的方法#cite("Rocki2011ParallelMC")。通过利用GPU的并行计算能力，他们加速了MCTS的搜索过程，提高了搜索效率，并在棋类AI中取得了显著的性能提升。

总体而言，通过蒙特卡洛搜索树算法，棋类人工智能取得了显著的进展。这些研究展示了MCTS方法的优势和潜力，能够帮助决策AI在不完全信息和不确定性的环境中做出优化的决策。然而，仍有一些挑战需要解决，例如搜索效率和对大规模游戏状态的处理。因此，进一步的研究仍然需要探索如何改进和扩展MCTS算法，以提高棋类AI的性能和智能水平。

在MCTS的发展过程的第二个阶段中，研究者们使用神经网络技术取代了先验知识。在这个阶段中，AlphaGo和AlphaZero作为里程碑式的成果引起了广泛的关注。AlphaGo的成功应用了MCTS算法和深度神经网络，实现了超人类水平的围棋对弈能力#cite("silver2016mastering")。而AlphaZero进一步发展了MCTS算法的应用，通过自我对弈和强化学习进行训练，取得了在围棋、国际象棋和将棋等游戏中卓越的表现#cite("silver2017mastering")。

综上所述，蒙特卡洛搜索树算法在棋类人工智能中的应用已经取得了重要的成果，为棋类游戏决策提供了一种有效的方法。这些研究为我们理解MCTS方法的优势和应用领域提供了深入的见解，并为未来的研究提供了有价值的参考。

= 理论和方法

== 问题建模

#hide("缩进")以五子棋游戏为例，我们首先对该问题进行建模。五子棋游戏大多使用15x15大小的棋盘，所以我们使用一个长度为225的向量作为棋盘的一个状态(state)。对于状态向量中的一个值，其取值有三种可能：

- 0：无棋子
- 1：该位置有黑棋
- 2：该位置有白棋

#hide("缩进")由此，我们将每个状态下的动作(action)空间，即可以落子的位置定义为棋盘上无棋子的空白位置。胜利条件被定义为某一方形成连续的五个棋子（横向、纵向、对角线）。特别的，当棋盘上已无空白位置，但又没有任何一方形成连续的五个棋子时，定义为平局。

额外的，为了能够通过引入先验知识来辅助决策，我们还选取了两个特征进行补充。即当前决策方，取值为1或2. 以及上一步的决策，取值为1到225的标量。

最后，一个决策由一个1到225之间的标量表示，其含义为在第(x mod 15,x/15)处落下一颗棋子。

== 蒙特卡洛树搜索
#hide("缩进")MCTS基于蒙特卡洛模拟和树搜索的思想，通过大量的随机模拟游戏对局来估计每个行动的价值，并根据这些估值来指导决策。它通过在搜索树中不断扩展和更新节点来保持对决策空间的探索，并逐渐收敛到最佳的行动选择。其主要算法流程如@MCTS流程图。

#figure(
    image("figs/MCTS流程图.png",height: 40%),
    caption: [MCTS流程图],
  )<MCTS流程图>
  
#hide("缩进")MCTS算法在运行时主要遵循以下几个步骤：
+ 初始化搜索树：从当前游戏状态开始，创建一个仅包含根节点的搜索树。

+ 选择阶段（Selection）：从根节点开始，通过一定策略选择子节点进行扩展。选择策略通常基于行动的价值估计。

+ 扩展阶段（Expansion）：对选中的子节点进行扩展，生成新的子节点，代表未探索的行动。

+ 模拟阶段（Simulation）：针对扩展节点，使用蒙特卡洛模拟进行游戏对局，直到达到终止条件。

+ 回溯阶段（Backpropagation）：根据模拟对局的结果，更新扩展节点及其祖先节点的统计信息，例如胜利次数和访问次数。

+ 重复选择、扩展、模拟和回溯阶段：通过重复执行上述步骤，逐渐扩展搜索树，增加对决策空间的探索。

+ 决策阶段：在给定的时间或迭代次数内，根据搜索树的统计信息，选择具有最高价值的行动作为最佳决策。

#hide("缩进")接下来，本文将分别介绍各个阶段的主要算法。

=== 初始化搜索树
定义搜索树的根节点，该节点状态为长度为225的零向量。

=== 选择子节点
#hide("缩进")在选择阶段，MCTS的目标是在搜索树中找到一个合适的子节点来进行扩展。该子节点应具有未完全探索的行动，并且具有较高的价值估计。通过选择具有潜在价值的子节点，MCTS能够在下一步的扩展中平衡探索未知行动和利用已知信息之间的权衡。在这一步的动作空间为棋盘上的所有空白位置，其伪代码描述如@选择子节点：

在选择阶段，通常会使用UCB1（Upper Confidence Bound 1）算法来选择最优子节点。UCB1算法是一种用于在蒙特卡洛树搜索算法（MCTS）中进行选择的策略。它综合考虑了已知信息的利益（exploitation）和探索未知行动的利益（exploration），以平衡在搜索过程中的探索和利用之间的权衡。UCB1算法通过计算每个子节点的UCB1值来进行选择，其中UCB1值较高的子节点被认为是更有希望的选择。UCB1算法基于以下原则：在选择过程中，我们应该优先选择那些在过去得到较少探索但看起来具有较高潜在价值的子节点。UCB1算法使用上置信界限来量化选择的准则，即它综合考虑了两个因素：探索次数和子节点的平均收益。UCB1算法的计算公式如：

$ "UCB1" = "平均收益" + "探索参数" * sqrt(log("总探索次数") / "当前子节点的探索次数") $
其中：

平均收益是指当前子节点的总收益除以探索次数。
探索参数是用于平衡探索和利用的调整因子。较大的探索参数促使算法更多地探索未知行动，较小的探索参数则更倾向于利用已知信息。

通过使用UCB1算法，MCTS能够在选择阶段中根据子节点的UCB1值来引导搜索，使其更倾向于选择那些在过去探索较少但看起来具有较高潜在价值的子节点。这种平衡的选择策略有助于提高MCTS在博弈和决策问题中的性能。
#figure(
  [```python
def select(node):
    while node is fully expanded and not terminal:
        node = UCB1_best_child(node)
    return node
def UCB1_best_child(node):
    best_child = null
    best_ucb1_value = -infinity
    for each child in children(node):
        ucb1_value = calculate_ucb1_value(child)
        if ucb1_value > best_ucb1_value:
            best_ucb1_value = ucb1_value
            best_child = child
    return best_child 
```],
    caption: [选择子节点],
  )<选择子节点>

=== 扩展结点
#hide("缩进")在扩展阶段，MCTS的目标是根据当前节点的状态和可行的行动，选择一个未探索的行动，创建一个新的子节点，并将搜索树扩展到该子节点。选择未被完全探索的行动后，可以生成一个新的子节点并将其添加到搜索树中。新子节点的状态根据选择的行动更新，即在当前状态下执行选定的行动，得到新的状态。该子节点的初始统计信息通常设置为零，例如访问次数和累计奖励。扩展阶段的伪代码如 @扩展节点 所示。

#figure(
  [```python
def expand(node):
    unexplored_actions = get_unexplored_actions(node)
    action = select_unexplored_action(unexplored_actions)
    new_state = apply_action(node.state, action)
    new_node = create_new_node(new_state)
    add_child_node(node, new_node, action)
    return new_node
```],
    caption: [扩展节点],
  )<扩展节点>
#hide("缩进")在扩展阶段的伪代码中，`expand`函数根据当前节点的状态选择一个未被完全探索的行动，并根据该行动生成一个新的子节点。`get_unexplored_actions`函数获取未被完全探索的行动列表。`select_unexplored_action`函数从未探索的行动中随机选择一个行动。`apply_action`函数执行选定的行动，生成新的状态。`create_new_node`函数创建一个新的节点，并将初始统计信息设置为零。`add_child_node`函数将新的子节点添加到父节点中。通过以上步骤，扩展阶段将搜索树扩展到新的未探索状态，并准备进行下一步的模拟或选择。通过不断重复扩展、模拟和选择阶段，MCTS能够逐步改善对游戏或决策问题的理解和预测能力，并找到最佳的行动策略。

=== 模拟游戏
#hide("缩进")在模拟阶段，MCTS的目标是评估未扩展节点的行动，以获得对每个行动的胜率或获胜概率的估计。通过执行大量的随机对局或模拟游戏，MCTS能够估计每个行动导致胜利的可能性，并用于更新搜索树中节点的统计信息。下面是模拟阶段的伪代码示例：

#figure(
  [```python
def simulate(node):
    state = node.state
    while not game_over(state):
        action = select_random_action(state)  // 随机选择一个行动
        state = apply_action(state, action)  // 执行行动，更新状态
    result = evaluate_game_result(state)  // 评估游戏结果
    return result
```],
    caption: [模拟游戏],
  )<模拟游戏>
#hide("缩进")在模拟阶段，通常采用随机策略或简单规则来模拟游戏的进行，并在最后根据游戏的结果（胜利、平局或失败），将模拟的结果反馈给扩展节点。

=== 回溯阶段

回溯阶段是蒙特卡洛树搜索算法（MCTS）中的最后一个步骤，用于更新搜索树中经过的节点的统计信息。在回溯阶段，MCTS将根据模拟阶段的结果，沿着搜索路径回溯到根节点，并更新每个经过的节点的访问次数和收益信息。其主要伪代码如@回溯阶段 ：

#figure(
  [```python
def backpropagate(node, result):
    while node is not null:
        update_node_stats(node, result)
        node = node.parent

def update_node_stats(node, result):
    node.visit_count += 1  // 增加访问次数
    node.total_reward += result  // 更新累计奖励
```],
    caption: [回溯阶段],
  )<回溯阶段>

#hide("缩进")在回溯阶段的伪代码中，backpropagate函数从模拟阶段的结束节点开始，通过回溯到父节点的方式，依次更新每个经过的节点的统计信息。update_node_stats函数更新节点的访问次数和累计奖励。对于每个节点，增加访问次数，并将模拟阶段的结果累加到累计奖励中。

通过回溯阶段，MCTS能够根据模拟的结果更新搜索树中经过的节点的统计信息，从而反映实际游戏或决策问题的情况。这样，在下一次选择和扩展阶段，MCTS可以利用更新后的统计信息来指导搜索，更准确地评估行动的潜在价值，并提高决策的质量。

=== 决策
#hide("缩进")重复上述步骤足够多次数后，在给定的时间或迭代次数内，根据搜索树的统计信息，选择具有最高价值的行动作为最佳决策。

== 深度神经网络
#hide("缩进")传统的MCTS方法在搜索过程中需要大量的模拟和评估，耗时较长。而结合神经网络后，可以利用神经网络对游戏状态进行快速评估，将搜索空间缩小到更有希望的候选行动上，从而提高搜索效率。在这里神经网络主要通过训练，能够“储存”MCTS的搜索结果，避免每次重新搜索，从而更加准确的判断局势。在本实验中，神经网络的主要作用是进行更加精确的分数计算，即不再通过大量模拟来确定一个行动的分数，而是使用神经网络直接计算。
=== 特征提取网络
#hide("缩进")特征提取网络的作用是将隐藏在棋局中的特征，其输入是一个(225,4)的矩阵，其四个分量分别表示黑子历史落点、白子历史落点、最后一步落子位置、最后一步落子选手。其输出是一个(225,128)的矩阵，这个大型矩阵隐含了棋局的特征信息。该层网络使用ReLU激活函数，包含三个卷积层。
=== 策略网络
#hide("缩进")策略网络的作用是给出下一步着子的概率分布，其输入是棋局的特征，即(225,128)的矩阵，输出是一个长度为225的向量，向量的每一个值都在0到1之间，且其L1范数为1.其含义是对下一步着子位置的建议程度。在着子时，我们根据这个矩阵和未着子位置来综合考虑下一步着子位置。该层网络使用ReLU激活函数，包含一个卷积层和一个全连接层。
=== 价值网络
#hide("缩进")价值网络的作用是给出当前选手的局面胜率，其输入仍然是棋局的特征，输出为一个在-1到1之间的标量。包含一个卷积层和两个全连接层。

=== 网络整体结构

#hide("缩进")综上所述，深度神经网络的整体结构图如@网络结构图 所示：

#figure(
    image("figs/网络结构图.svg",height: 36%),
    caption: [网络结构图],
  )<网络结构图>

#hide("缩进")在网络中，策略生成网络最后需要得到概率分布，故而使用`softmax`进行归一化；而对于价值评估网络，为了保证最后生成合理的胜率，则使用`tanh`来作为激活函数。
  
= 实验结果和分析

== 实验环境

#hide("缩进")本次实验所使用的主要编程语言为`Python`。所依赖的主要库及版本对应如 @实验环境依赖表 。本次实验的硬件设备参数如@设备参数表。
#figure(
  table(
  columns: (auto, auto),
  inset: 10pt,
  align: horizon,
  [*依赖环境*], [*版本*],
  [Python],[3.11.3],
  [numpy],[1.24.3],
  [torch],[2.0.1],
  [gym-gomoku],[0.26.2]
  
),
    caption: [实验环境依赖表],
  )<实验环境依赖表>

#figure(
  table(
  columns: (auto, auto),
  inset: 10pt,
  align: horizon,
  [*硬件设备*], [*参数*],
  [CPU],[Intel i-12700],
  [Windows],[11],
  [内存],[16GB],
  [硬盘],[500GB]
),
    caption: [设备参数表],
  )<设备参数表>

== 主要参数设置及训练过程
#hide("缩进")由于硬件设备和时间限制，本次没有进行完整的五子棋AI训练，实验主要包含两个部分。分别是井字棋和在6x6大小棋盘上的四子棋。本次实验所设置的主要参数包括：
- 学习率：0.002
- L2正则化参数：0.0001
- 训练轮次：300（井字棋）、1200（四子棋）
- MCTS搜索轮次：600

== 实验结果
#hide("缩进")对井字棋训练300轮次，对四子棋AI训练1200轮次，与学生的对弈结果分别如@井字棋对弈、 @4子棋对弈。

可以观察到，井字棋规则比较简单，且有非常简单的平局策略，所以仅需要训练300轮次就可以完成AI的训练（事实上，可能不需要训练300轮次）。值得一提的是，由于AI是通过自我对弈进行训练，所以AI已经默认了该游戏的结果是平局，所以当AI进行先手落子时，并没有选择在棋盘正中心落子。这可能是由于不论在哪里落子，都是100%平局导致的。

另外，对于较复杂的4子棋，就可以看出一定的变化。例如学生在和训练300轮次的AI对弈时，可以坚持到AI落下第九颗子才落败。而和训练了1200轮次的AI对弈时，最多只能坚持到第7颗子。

#figure(
    image("figs/井字棋_对弈.png",height: 30%),
    caption: [井字棋对弈结果图],
  )<井字棋对弈>

  #figure(
    image("figs/4子棋_对弈.png",height: 55%),
    caption: [4子棋对弈结果图],
  )<4子棋对弈>

== 复现方法
=== 代码下载方式
#hide("缩进")本实验可以在任何一台Windows电脑上进行复现（理论上Linux服务器上也是可以的）。本实验的所有代码以及本报告的`typst`源码可以在#link("https://github.com/qisumi/Advanced-Statistical-Optimization-Algorithm-Report")[Github]或者#link("https://pan.seu.edu.cn:443/link/568710E74C4AE47B0203D026AD006480")[东南大学云盘]上下载。
=== 环境配置方式
#hide("缩进")假设使用Windows电脑进行试验，只需要下载3.8以上版本的Python，同时安装@实验环境依赖表 内的包，就完成了环境的配置。默认情况下，本实验不需要GPU，只需要CPU即可运行。
===  训练和对弈
如果你需要进行进行井字棋对弈，那么只需运行
```shell-unix-generic
python human_play.py
```

如果你需要进行4子棋对弈，那么只需运行
```shell-unix-generic
python dist/human_play_6x6.py
```

#hide("缩进")如果你需要进行训练，首先需要修改`train.py`中的参数，其中`board_size`为棋盘的大小，`n_inrow`表示胜利所需要连成几颗棋子。之后，一些训练相关的参数可以在`Model/MCTAgent.py`的126到137行进行设定。除此之外，你还需要在项目目录创建文件夹`checkpoints`用来保存中间模型。最后运行如下代码开始训练：
```shell-unix-generic
python train.py
```

#hide("缩进")根据经验，井字棋AI大约只需要15分钟即可训练完成，而四子棋AI大约需要4到5个小时。根据测试，由于代码的性能瓶颈主要在MCTS实现上，所以使用GPU服务器并不会显著的提速训练。学生尝试使用`RTX TITAN`进行训练，并没有明显的提升。

= 总结
#hide("缩进")蒙特卡洛算法在解决四子棋博弈决策中具有一定的应用潜力，但也存在一些局限性。在面对复杂的博弈游戏时，搜索空间会变得非常庞大，导致计算复杂性的增加。对于五子棋这样的游戏，学生受条件限制，就没有进行训练（完成一个可用的AI训练可能需要数周）。尤其在结合强化学习和神经网络后，通常需要大量的训练数据来优化模型和提升决策能力。在五子棋博弈中，为了达到高水平的决策能力，需要进行大量的自我对弈和训练，这可能需要很长的时间和计算资源。

同时蒙特卡洛算法在搜索过程中往往只考虑局部最优解，可能会陷入局部最优策略而无法找到全局最优解。在实际应用中，如果存在一些看似优势的走法，但实际上存在更好的决策，蒙特卡洛算法可能会受限于其随机性而无法发现。例如在井字棋游戏中，第一步棋下载棋盘中央是显而易见的最优解，但是AI并没有这样做。

在本门课程的学习中，学生受益良多。学生来自计算机学院，主修方向是时序数据预测。所以在本门课程中，对我更有帮助的是学习到了Bootstrap方法和一些缺失样本插补方法。但是由于这些方法单独使用难以支撑一个完整的实验，故而学生选择了蒙特卡洛算法在计算机中较有代表性也比较有趣的应用进行实验。

#pagebreak()
#bibliography("ref.bib")